import express from 'express';
import dotenv from 'dotenv';
dotenv.config();

import cors from 'cors';
import multer from 'multer';
import fs from 'fs-extra';
import path from 'path';
import JSZip from 'jszip';
import { glob } from 'glob';
import { v4 as uuidv4 } from 'uuid';
import { spawn } from 'child_process';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import { WebSocketServer } from 'ws';

import { UltraHonkBackend } from '@aztec/bb.js';
import { Buffer } from 'buffer';

const VERIFY_SCRIPT_CONTENT = `/* * Remix Script Runner Configuration (Required before running!):
 * 1. Click the dropdown arrow next to the 'Run script' button in the top left of the editor.
 * 2. Select 'Open script configuration'.
 * 3. In the configuration modal or panel, choose 'Noir' as the execution environment.
 * 4. Close the configuration and then click 'Run script'.
 */
/* eslint-disable @typescript-eslint/no-var-requires */
const { expect } = require('chai');
import { UltraHonkBackend } from '@aztec/bb.js';

// A utility function to convert a hexadecimal string (with or without '0x' prefix) into a Uint8Array.
// bb.js requires proof data to be in a byte array format, not a string.
const hexToBytes = (hex) => {
  if (typeof hex !== 'string') {
    throw new Error('Input must be a string');
  }
  if (hex.startsWith('0x')) {
    hex = hex.slice(2);
  }
  if (hex.length % 2 !== 0) {
    hex = '0' + hex;
  }
  const bytes = new Uint8Array(hex.length / 2);
  for (let i = 0; i < bytes.length; i++) {
    bytes[i] = parseInt(hex.slice(i * 2, i * 2 + 2), 16);
  }
  return bytes;
};

describe('JS Verification of Backend Proof', () => {
  let backend;
  let proofBytes;
  let publicInputs;

  const BUILD_PATH = '%%BUILD_PATH%%';

  before(async () => {
    console.log('Loading artifacts generated by the backend...');
    
    // Step 1: Load the compiled circuit artifact (\`program.json\`).
    // This file contains the essential \`bytecode\` needed to initialize the verifier backend.
    // We are using the result from the backend.
    const programJson = await remix.call('fileManager', 'readFile', BUILD_PATH + '/program.json');
    const circuit = JSON.parse(programJson);
    if (!circuit.bytecode) {
      throw new Error("program.json does not contain 'bytecode'.");
    }

    // Step 2: Load the proof and public inputs that were generated and formatted by the backend.
    // These are the specific artifacts we want to verify.
    const proofHex = await remix.call('fileManager', 'readFile', BUILD_PATH + '/proof');
    const publicInputsJson = await remix.call('fileManager', 'readFile', BUILD_PATH + '/public_inputs');

    // Step 3: Prepare the loaded data for use with bb.js.
    // The proof is converted from a hex string to bytes, and the public inputs are parsed from a JSON string.
    proofBytes = hexToBytes(proofHex);
    publicInputs = JSON.parse(publicInputsJson);
    
    console.log(\`Proof size: \${proofBytes.length} bytes\`);
    console.log(\`Public inputs: \${publicInputs.length}\`);

    // Step 4: Initialize the UltraHonkBackend with the circuit's bytecode.
    // This backend instance will perform the actual verification calculation.
    backend = new UltraHonkBackend(circuit.bytecode);
  });

  after(async () => {
    if (backend) {
      console.log('Destroying backend...');
      await backend.destroy();
    }
  });

  it('should verify the proof successfully', async () => {
    console.log('Verifying proof using bb.js...');
    
    // Step 5: Call the \`verifyProof\` method.
    // It takes an object containing the \`proof\` (as bytes) and \`publicInputs\`.
    // The \`{ keccak: true }\` option is crucial and must match the option used on the backend
    // when the proof was generated to ensure compatibility.
    const verified = await backend.verifyProof({ proof: proofBytes, publicInputs }, { keccak: true });
    
    console.log('---------------------------');
    console.log(\`Verification result: \${verified}\`);
    console.log('---------------------------');

    // Use the Chai \`expect\` function to assert that the verification result is true.
    // If \`verified\` is false, the test will fail and display the error message
    expect(verified, 'Proof fails verification in JS').to.be.true;
  });
});
`;

const garagaPath = '/home/ubuntu/garaga-venv/bin/garaga';
// const garagaPath = '/Users/sooyounghyun/Desktop/dev/garaga/venv/bin/garaga';

// ---------------- WebSocket setup ----------------
const wss = new WebSocketServer({ port: 8082, path: '/ws/' });
const wsClients = new Map();

wss.on('connection', (ws) => {
  ws.on('message', (msg) => {
    try {
      const { requestId } = JSON.parse(msg);
      if (requestId) {
        wsClients.set(requestId, ws);
        ws.send(JSON.stringify({ logMsg: `[WS] Bound to requestId: ${requestId}` }));
      }
    } catch (e) {
      console.error('[WS] Invalid message from client:', msg);
    }
  });

  ws.on('close', () => {
    for (const [key, client] of wsClients.entries()) {
      if (client === ws) wsClients.delete(key);
    }
  });
});

function sendLog(requestId, msg) {
  try {
    const ws = wsClients.get(requestId);
    if (ws && ws.readyState === ws.OPEN) {
      ws.send(JSON.stringify({ logMsg: msg }));
    } else {
      console.warn(`[sendLog] No active WebSocket for requestId=${requestId}`);
    }
  } catch (e) {
    console.error(`[sendLog] Failed to send log for requestId=${requestId}:`, e.message);
  }
}

// ---------------- Env & Upload ----------------

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const upload = multer({
  storage: multer.memoryStorage(),
  limits: { fileSize: 20 * 1024 * 1024 },
});

const app = express();
app.use(express.json());
app.use(express.urlencoded({ extended: true }));
app.use(cors({
  origin: '*',
  methods: ['GET', 'POST'],
  allowedHeaders: ['Content-Type'],
}));
app.use((err, req, res, next) => {
  if (err instanceof multer.MulterError && err.code === 'LIMIT_FILE_SIZE') {
    return res.status(413).json({ success: false, error: 'Uploaded file is too large (max 20MB).' });
  }
  next(err);
});

// ---------------- Run command ----------------
const run = (cmd, args, cwd, requestId) => new Promise((resolve, reject) => {
  const proc = spawn(cmd, args, {
    cwd,
    shell: true,
    env: {
      ...process.env,
      PATH: [
        process.env.PATH,
        process.platform === 'darwin'
          ? `${process.env.HOME}/.nargo/bin:${process.env.HOME}/.bb`
          : '/home/ubuntu/.nargo/bin:/home/ubuntu/.bb'
      ].join(':')
    }
  });

  let stderrLog = '';
  let stdoutLog = '';

  proc.stdout.on('data', (data) => {
    const log = data.toString().trim();
    stdoutLog += log + '\n';
    console.log(`[${cmd}] stdout:`, log);
    if (requestId) sendLog(requestId, `[${cmd}] stdout: ${log}`);
  });

  proc.stderr.on('data', (data) => {
    const log = data.toString().trim();
    stderrLog += log + '\n';
    console.error(`[${cmd}] stderr:`, log);
    if (requestId) sendLog(requestId, `[${cmd}] stderr: ${log}`);
  });

  proc.on('close', (code) => {
    const msg = `[${cmd}] Completed with exit code ${code}`;
    if (requestId) sendLog(requestId, msg);
    code === 0 ? resolve() : reject(new Error(`${msg}\n${stderrLog}`));
  });

  proc.on('error', (error) => {
    const msg = `[${cmd}] Error: ${error.message}`;
    if (requestId) sendLog(requestId, msg);
    reject(error);
  });
});


// ---------------- Extract zip ----------------
const extractZipStripRoot = async (zipBuffer, targetPath) => {
  const zip = await JSZip.loadAsync(zipBuffer);
  const entries = Object.keys(zip.files);
  const entryPaths = entries.filter(e => !e.endsWith('/'));

  let rootPrefix = '';
  if (entryPaths.length > 0) {
    const firstPathParts = entryPaths[0].split('/');
    if (firstPathParts.length > 1) {
      const candidatePrefix = firstPathParts[0] + '/';
      const allHavePrefix = entryPaths.every(p => p.startsWith(candidatePrefix));
      if (allHavePrefix) {
        rootPrefix = candidatePrefix;
        console.log(`[extractZip] Removing common root prefix: "${rootPrefix}"`);
      }
    }
  }

  for (const filename of entries) {
    try {
      const file = zip.files[filename];
      const strippedPath = rootPrefix ? filename.replace(rootPrefix, '') : filename;
      if (!strippedPath) continue;

      const fullPath = path.join(targetPath, strippedPath);
      const normalizedPath = path.normalize(fullPath);
      if (!normalizedPath.startsWith(targetPath)) {
        throw new Error(`Unsafe file path detected: ${filename}`);
      }

      if (file.dir) {
        await fs.mkdirp(normalizedPath);
      } else {
        await fs.mkdirp(path.dirname(normalizedPath));
        const content = await file.async('nodebuffer');
        await fs.writeFile(normalizedPath, content);
      }
    } catch (err) {
      console.warn(`[extractZip] Failed to extract "${filename}":`, err.message);
    }
  }
};

// ---------------- Health check ----------------

app.get('/', (req, res) => {
  res.send('Noir backend is running');
});

// ---------------- /compile ----------------
app.post('/compile', upload.single('file'), async (req, res) => {
  const requestId = req.query.requestId || uuidv4();
  const projectPath = path.join(__dirname, 'uploads', requestId);
  
  console.log(`[${requestId}] New compile request.`);
  
  const zipBuffer = req.file?.buffer;
  if (!zipBuffer) return res.status(400).json({ success: false, error: 'No file provided' });

  try {
    await fs.mkdirp(projectPath);
    await extractZipStripRoot(zipBuffer, projectPath);

    sendLog(requestId, '[debug] --- Extracted File Structure ---');
    await run('ls', ['-R'], projectPath, requestId); 
    sendLog(requestId, '[debug] ---------------------------------');

    const nargoTomlPaths = await glob(path.join(projectPath, '**/Nargo.toml'));
    
    sendLog(requestId, `[debug] Nargo.toml search result: ${JSON.stringify(nargoTomlPaths)}`);

    if (nargoTomlPaths.length === 0) {
      throw new Error('Nargo.toml not found in the uploaded project.');
    }
    const rootDir = path.dirname(nargoTomlPaths[0]);

    sendLog(requestId, `[debug] Determined project root (cwd) for nargo: ${rootDir}`);

    await run('nargo', ['compile'], rootDir, requestId);
    await run('nargo', ['check'], rootDir, requestId);

    const targetDir = path.join(rootDir, 'target');
    const files = await fs.readdir(targetDir);
    const jsonFile = files.find(f => f.endsWith('.json'));
    if (!jsonFile) throw new Error('Compiled JSON file not found');

    const json = await fs.readFile(path.join(targetDir, jsonFile), 'utf8');
    const proverTomlPath = path.join(rootDir, 'Prover.toml');
    const prover = await fs.pathExists(proverTomlPath)
      ? await fs.readFile(proverTomlPath, 'utf8')
      : '';

    sendLog(requestId, 'Compilation succeeded!');
    res.json({ success: true, requestId, compiledJson: json, proverToml: prover });
  } catch (e) {
    console.error(`[${requestId}] Compile Error:`, e);
    sendLog(requestId, `Compilation failed: ${e.message}`);
    res.status(500).json({ success: false, error: e.message });
  } finally {
    await fs.remove(projectPath).catch(err => console.error('cleanup error:', err));
  }
});

// -------------------- /generate-proof-with-verifier --------------------
app.post('/generate-proof-with-verifier', upload.single('file'), async (req, res) => {
  const requestId = req.query.requestId || uuidv4();
  const zip = new JSZip();
  const projectPath = path.join(__dirname, 'uploads', requestId);
  
  const zipBuffer = req.file?.buffer;
  if (!zipBuffer) return res.status(400).json({ success: false, error: 'No file provided' });

  let rootDir = '';
  let targetDir = '';
  let jsonFile = '';
  let backend = null;

  try {
    await fs.mkdirp(projectPath);
    await extractZipStripRoot(zipBuffer, projectPath);

    const proverPaths = await glob(path.join(projectPath, '**/Prover.toml'));
    if (proverPaths.length === 0) throw new Error('Prover.toml not found in uploaded zip');
    rootDir = path.dirname(proverPaths[0]);

    await run('nargo', ['execute'], rootDir, requestId);

    targetDir = path.join(rootDir, 'target');
    const files = await fs.readdir(targetDir);
    jsonFile = files.find(f => f.endsWith('.json'));
    if (!jsonFile) throw new Error('Compiled circuit JSON not found');

    const gzFile = files.find(f => f.endsWith('.gz'));
    if (!gzFile) throw new Error('Witness file (.gz) not found');
    const witnessFile = path.join(targetDir, gzFile);

    sendLog(requestId, '[debug] Reading artifact and witness buffers...');
    const circuitJson = JSON.parse(await fs.readFile(path.join(targetDir, jsonFile), 'utf8'));
    const witnessBuffer = await fs.readFile(witnessFile);

    sendLog(requestId, '[debug] Initializing UltraHonkBackend...');
    backend = new UltraHonkBackend(circuitJson.bytecode);

    sendLog(requestId, '[debug] Generating proof via bb.js...');
    const proofData = await backend.generateProof(witnessBuffer, { keccak: true });

    if (!proofData || !proofData.proof || proofData.proof.length === 0) {
      throw new Error('bb.js backend.generateProof() returned empty or invalid proof data.');
    }
    sendLog(requestId, '[debug] Proof data generated successfully.');

    sendLog(requestId, '[debug] Getting verification key via bb.js...');
    const vkBin = await backend.getVerificationKey({ keccak: true });

    if (!vkBin || vkBin.length === 0) {
      throw new Error('bb.js backend.getVerificationKey() returned empty verification key.');
    }
    sendLog(requestId, '[debug] Verification key generated successfully.');

    sendLog(requestId, '[debug] Getting Solidity verifier via bb.js...');
    const verifierSol_Bin = await backend.getSolidityVerifier(vkBin);

    if (!verifierSol_Bin || verifierSol_Bin.length === 0) {
      throw new Error('bb.js backend.getSolidityVerifier() returned empty verifier contract.');
    }

    const verifierSol = Buffer.from(verifierSol_Bin).toString('utf8');

    if (!verifierSol || verifierSol.trim().length === 0) {
        throw new Error('Solidity verifier content is empty after buffer conversion.');
    }
    sendLog(requestId, '[debug] Solidity verifier generated successfully.');
    
    const formattedProof = '0x' + Buffer.from(proofData.proof).toString('hex');
    const formattedPublicInputs = proofData.publicInputs.map(
      (input) => input.toString().padStart(64, '0')
    );

    zip.file('vk', Buffer.from(vkBin));
    zip.file('verifier/solidity/Verifier.sol', verifierSol);
    zip.file('program.json', JSON.stringify(circuitJson, null, 2));
    
    zip.file('proof', formattedProof);
    zip.file('public_inputs', JSON.stringify(formattedPublicInputs, null, 2));

    zip.file('scripts/verify.ts', VERIFY_SCRIPT_CONTENT);

    const zipBufferOut = await zip.generateAsync({ type: 'nodebuffer' });
    res.set('Content-Type', 'application/zip');
    res.set('Content-Disposition', `attachment; filename=verifier_${requestId}.zip`);
    res.send(zipBufferOut);

  } catch (e) {
    console.error(`[${requestId}] generate-proof-with-verifier Error:`, e);
    sendLog(requestId, `generate-proof-with-verifier failed: ${e.message}`);
    res.status(500).json({ 
      success: false, 
      error: e.message,
      logs: `Error occurred in ${rootDir} (target: ${targetDir}, artifact: ${jsonFile})` 
    });
  } finally {
    if (backend) {
      await backend.destroy();
      sendLog(requestId, '[debug] bb.js backend destroyed.');
    }
    await fs.remove(projectPath).catch(err => console.error('cleanup error:', err));
  }
});

// ---------------- Start ----------------
app.listen(3000, () => {
  console.log('Noir backend running on port 3000');
});

// -------------------- /compile-with-profiler --------------------
// Note: Images generated by the profiler cannot be viewed in the Monaco editor.
app.post('/compile-with-profiler', upload.single('file'), async (req, res) => {
  const requestId = req.query.requestId || uuidv4();
  const profilers = (req.query.profiler || '').split(',').filter(Boolean);
  const zip = new JSZip();
  const projectPath = path.join(__dirname, 'uploads', requestId);
  console.log('Uploaded file size:', req.file?.size, 'bytes');
  try {
    await fs.mkdirp(projectPath);

    const zipBuffer = req.file?.buffer;
    if (!zipBuffer) throw new Error('No file uploaded');

    await extractZipStripRoot(zipBuffer, projectPath);

    await run('nargo', ['compile'], projectPath, requestId);
    await run('nargo', ['check'], projectPath, requestId);

    const targetDir = path.join(projectPath, 'target');
    const jsonFile = (await fs.readdir(targetDir)).find(f => f.endsWith('.json'));
    if (!jsonFile) throw new Error('Compiled JSON not found');

    zip.file(`compiled/${jsonFile}`, await fs.readFile(path.join(targetDir, jsonFile)));

    // ACIR opcode profiling only
    if (profilers.includes('acir')) {
      sendLog(requestId, '[profiler] Running ACIR opcode profiler...');
      await run('noir-profiler', [
        'opcodes',
        '--artifact-path', `./target/${jsonFile}`,
        '--output', './target'
      ], projectPath, requestId);
    }

    // Collect .svg profiler output
    const svgFiles = await glob(path.join(targetDir, '*_opcodes.svg'));
    for (const filePath of svgFiles) {
      const name = path.basename(filePath);
      zip.file(`profiler/${name}`, await fs.readFile(filePath));
    }

    const zipBufferOut = await zip.generateAsync({ type: 'nodebuffer' });
    res.set('Content-Type', 'application/zip');
    res.set('Content-Disposition', `attachment; filename=compile_profiler_${requestId}.zip`);
    res.send(zipBufferOut);
  } catch (e) {
    console.error('[compile-with-profiler] Error:', e);
    sendLog(requestId, `compile-with-profiler failed: ${e.message}`);
    res.status(500).json({ success: false, error: e.message });
  } finally {
    await fs.remove(projectPath).catch(err => console.error('cleanup error:', err));
  }
});

// ---------------- /generate-proof ----------------
// ⚠️ [DEPRECATED] This endpoint is still used in production but is scheduled for removal.
// Use `/generate-proof-with-verifier` instead for support with optional Starknet & profiling features.
app.post('/generate-proof', upload.single('file'), async (req, res) => {
  const requestId = req.query.requestId || uuidv4();
  const zipBuffer = req.file?.buffer;
  const projectPath = path.join(__dirname, 'uploads', requestId);

  if (!zipBuffer) return res.status(400).json({ success: false, error: 'No file provided' });

  try {
    await fs.mkdirp(projectPath);
    await extractZipStripRoot(zipBuffer, projectPath);

    const proverPaths = await glob(path.join(projectPath, '**/Prover.toml'));
    if (proverPaths.length === 0) throw new Error('Prover.toml not found in uploaded zip');
    const proverPath = proverPaths[0];
    sendLog(requestId, `[debug] Found Prover.toml at: ${proverPath}`);

    await run('nargo', ['execute'], projectPath, requestId);

    const targetDir = path.join(projectPath, 'target');
    const files = await fs.readdir(targetDir);
    const jsonFile = files.find(f => f.endsWith('.json'));
    if (!jsonFile) throw new Error('Compiled circuit JSON not found in target/');

    const gzFile = files.find(f => f.endsWith('.gz'));
    if (!gzFile) throw new Error('Witness file (.gz) not found in target/');
    const witnessFile = `target/${gzFile}`;
    sendLog(requestId, `[generate-proof] Using witness file: ${witnessFile}`);

    await run('bb', ['prove', '-b', `target/${jsonFile}`, '-w', witnessFile, '-o', 'target'], projectPath, requestId);
    await run('bb', ['write_vk', '-b', `target/${jsonFile}`, '-o', 'target', '--oracle_hash', 'keccak'], projectPath, requestId);
    await run('bb', ['write_solidity_verifier', '-k', 'target/vk', '-o', 'target/Verifier.sol'], projectPath, requestId);

    const proof = await fs.readFile(path.join(targetDir, 'proof'), 'utf8');
    const vk = await fs.readFile(path.join(targetDir, 'vk'), 'utf8');
    const verifier = await fs.readFile(path.join(targetDir, 'Verifier.sol'), 'utf8');

    sendLog(requestId, 'Proof + Verifier generated successfully.');
    res.json({ success: true, requestId, proof, vk, verifier });
  } catch (e) {
    console.error('[generate-proof] Error:', e);
    sendLog(requestId, `generate-proof failed: ${e.message}`);
    res.status(500).json({ success: false, error: e.message });
  } finally {
    await fs.remove(projectPath).catch(err => console.error('cleanup error:', err));
  }
});

